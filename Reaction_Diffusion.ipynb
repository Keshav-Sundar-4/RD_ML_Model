{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrNmNjxhAxXpTvIvYT5nuD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keshav-Sundar-4/RD_ML_Model/blob/main/Reaction_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports and Notebook Utilities\n",
        "import os\n",
        "import io\n",
        "import PIL.Image, PIL.ImageDraw\n",
        "import base64\n",
        "import zipfile\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import glob\n",
        "from scipy import ndimage\n",
        "from tqdm import tnrange\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "\n",
        "#Pytorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import ToTensor\n",
        "print(torch.__version__)\n",
        "\n",
        "#Video and Image imports\n",
        "from IPython.display import Image, HTML, clear_output\n",
        "import tqdm\n",
        "\n",
        "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
        "import moviepy.editor as mvp\n",
        "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
        "\n",
        "\n",
        "def imread(url, max_size=None, mode=None):\n",
        "  if url.startswith(('http:', 'https:')):\n",
        "    r = requests.get(url)\n",
        "    f = io.BytesIO(r.content)\n",
        "  else:\n",
        "    f = url\n",
        "  img = PIL.Image.open(f)\n",
        "  if max_size is not None:\n",
        "    img.thumbnail((max_size, max_size), PIL.Image.ANTIALIAS)\n",
        "  if mode is not None:\n",
        "    img = img.convert(mode)\n",
        "  img = np.float32(img)/255.0\n",
        "  return img\n",
        "\n",
        "def np2pil(a):\n",
        "  if a.dtype in [np.float32, np.float64]:\n",
        "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def imwrite(f, a, fmt=None):\n",
        "  a = np.asarray(a)\n",
        "  if isinstance(f, str):\n",
        "    fmt = f.rsplit('.', 1)[-1].lower()\n",
        "    if fmt == 'jpg':\n",
        "      fmt = 'jpeg'\n",
        "    f = open(f, 'wb')\n",
        "  np2pil(a).save(f, fmt, quality=95)\n",
        "\n",
        "def imencode(a, fmt='jpeg'):\n",
        "  a = np.asarray(a)\n",
        "  if len(a.shape) == 3 and a.shape[-1] == 4:\n",
        "    fmt = 'png'\n",
        "  f = io.BytesIO()\n",
        "  imwrite(f, a, fmt)\n",
        "  return f.getvalue()\n",
        "\n",
        "def im2url(a, fmt='jpeg'):\n",
        "  encoded = imencode(a, fmt)\n",
        "  base64_byte_string = base64.b64encode(encoded).decode('ascii')\n",
        "  return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string\n",
        "\n",
        "def imshow(a, fmt='jpeg'):\n",
        "  display(Image(data=imencode(a, fmt)))\n",
        "\n",
        "def tile2d(a, w=None):\n",
        "  a = np.asarray(a)\n",
        "  if w is None:\n",
        "    w = int(np.ceil(np.sqrt(len(a))))\n",
        "  th, tw = a.shape[1:3]\n",
        "  pad = (w-len(a))%w\n",
        "  a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
        "  h = len(a)//w\n",
        "  a = a.reshape([h, w]+list(a.shape[1:]))\n",
        "  a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
        "  return a\n",
        "\n",
        "def zoom(img, scale=4):\n",
        "  img = np.repeat(img, scale, 0)\n",
        "  img = np.repeat(img, scale, 1)\n",
        "  return img\n",
        "\n",
        "class VideoWriter:\n",
        "  def __init__(self, filename='_tmp.mp4', fps=30.0, **kw):\n",
        "    self.writer = None\n",
        "    self.params = dict(filename=filename, fps=fps, **kw)\n",
        "\n",
        "  def add(self, img):\n",
        "    img = np.asarray(img)\n",
        "    if self.writer is None:\n",
        "      h, w = img.shape[:2]\n",
        "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
        "    if img.dtype in [np.float32, np.float64]:\n",
        "      img = np.uint8(img.clip(0, 1)*255)\n",
        "    if len(img.shape) == 2:\n",
        "      img = np.repeat(img[..., None], 3, -1)\n",
        "    self.writer.write_frame(img)\n",
        "\n",
        "  def close(self):\n",
        "    if self.writer:\n",
        "      self.writer.close()\n",
        "\n",
        "  def __enter__(self):\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, *kw):\n",
        "    self.close()\n",
        "    if self.params['filename'] == '_tmp.mp4':\n",
        "      self.show()\n",
        "\n",
        "  def show(self, **kw):\n",
        "      self.close()\n",
        "      fn = self.params['filename']\n",
        "      display(mvp.ipython_display(fn, **kw))\n",
        "\n",
        "\n",
        "class LoopWriter(VideoWriter):\n",
        "  def __init__(self, *a, **kw):\n",
        "    super().__init__(*a, **kw)\n",
        "    self._intro = []\n",
        "    self._outro = []\n",
        "    self.fade_len = int(kw.get('fade_len', 1.0)*self.params['fps'])\n",
        "\n",
        "  def add(self, img):\n",
        "    if len(self._intro) < self.fade_len:\n",
        "      self._intro.append(img)\n",
        "      return\n",
        "    self._outro.append(img)\n",
        "    if len(self._outro) > self.fade_len:\n",
        "      super().add(self._outro.pop(0))\n",
        "\n",
        "  def close(self):\n",
        "    for t in np.linspace(0, 1, len(self._intro)):\n",
        "      img = self._intro.pop(0)*t + self._outro.pop(0)*(1.0-t)\n",
        "      super().add(img)\n",
        "    super().close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "aTb9ooYuRV2v",
        "outputId": "5b4237b0-396b-4815-9e4b-c3a2f78e1e37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.0+cu121\n",
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/google-research/self-organising-systems/raw/master/assets/reaction_diffusion_textures/targets.zip && unzip targets.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QBsSjbtqwo2",
        "outputId": "5c57b1c2-b207-433c-ae76-8802bab3012c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-07 18:07:13--  https://github.com/google-research/self-organising-systems/raw/master/assets/reaction_diffusion_textures/targets.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/google-research/self-organising-systems/master/assets/reaction_diffusion_textures/targets.zip [following]\n",
            "--2024-09-07 18:07:13--  https://raw.githubusercontent.com/google-research/self-organising-systems/master/assets/reaction_diffusion_textures/targets.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63479 (62K) [application/zip]\n",
            "Saving to: ‘targets.zip’\n",
            "\n",
            "targets.zip         100%[===================>]  61.99K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-09-07 18:07:13 (3.05 MB/s) - ‘targets.zip’ saved [63479/63479]\n",
            "\n",
            "Archive:  targets.zip\n",
            "  inflating: banded_0037_target.jpg  \n",
            "  inflating: chequered_0121_target.jpg  \n",
            "  inflating: grid_0135_target.jpg    \n",
            "  inflating: interlaced_0172_target.jpg  \n",
            "  inflating: lined_0118_target.jpg   \n",
            "  inflating: lizard_target.jpg       \n",
            "  inflating: polka-dotted_0121_target.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RD Model definition {vertical-output: false}\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Number of grid \"chemical\" channels\n",
        "CHN = 32\n",
        "\n",
        "# THIS FUNCTION CREATES A STARTING \"SEED\" STATE\n",
        "def seed_f(n, sz=96, spot_prob=0.005, spread=3.0):\n",
        "    '''Create seed states with scattered gaussian blobs in PyTorch'''\n",
        "    # Generate uniform random numbers and threshold them to create initial spots\n",
        "    x = torch.rand(n, sz, sz, 1) < spot_prob\n",
        "    x = x.float()  # Convert boolean tensor to float\n",
        "\n",
        "    # Apply Gaussian filter; note that PyTorch doesn't support wrapping mode directly\n",
        "    # If wrapping is crucial, you may need to implement it manually or adjust padding strategies\n",
        "    x = torch.from_numpy(gaussian_filter(x.numpy(), [0.0, spread, spread, 0.0], mode='wrap'))\n",
        "\n",
        "    # Scaling by spread squared\n",
        "    x *= spread**2\n",
        "\n",
        "    # Repeat the tensor along the channel dimension\n",
        "    x = x.repeat(1, 1, 1, 3)\n",
        "\n",
        "    # Pad the tensor to match the desired channel size (assuming 'CHN' is defined)\n",
        "    if x.shape[-1] < CHN:\n",
        "        padding = (0, CHN - x.shape[-1])  # Only pad the channel dimension\n",
        "        x = F.pad(x, padding)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "#THIS FUNCTION PROVIDES A PADDING OF P TO THE ENTERED TENSOR\n",
        "def pad_repeat(x, pad=1):\n",
        "    # Pad along the second dimension (height)\n",
        "    top = x[:, -pad:, :, :]  # Get the last 'pad' rows\n",
        "    bottom = x[:, :pad, :, :]  # Get the first 'pad' rows\n",
        "    x = torch.cat([top, x, bottom], dim=1)  # Concatenate along the height\n",
        "\n",
        "    # Pad along the third dimension (width)\n",
        "    left = x[:, :, -pad:, :]  # Get the last 'pad' columns\n",
        "    right = x[:, :, :pad, :]  # Get the first 'pad' columns\n",
        "    x = torch.cat([left, x, right], dim=2)  # Concatenate along the width\n",
        "\n",
        "    return x\n",
        "\n",
        "#THIS FUNCTION TAKES THE FIRST 3 CHANNEL VALUS AS RGB\n",
        "def to_rgb(x):\n",
        "  #Why is it adding 0.5?\n",
        "  return x[...,:3]+0.5\n",
        "\n",
        "#CREATES A LAPLACIAN KERNAL AND CONVOLVES IT\n",
        "def laplacian(x):\n",
        "    lap = torch.tensor([[1.0, 2.0, 1.0], [2.0, -12.0, 2.0], [1.0, 2.0, 1.0]]) / 16.0\n",
        "    lap = lap.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, 3, 3]\n",
        "    lap = lap.repeat(x.shape[1], 1, 1, 1)  # Repeat the kernel for each channel\n",
        "    x = pad_repeat(x, 1)  # Apply padding as defined previously\n",
        "    y = F.conv2d(x, lap, groups=x.shape[1], padding=0)  # Depthwise convolution\n",
        "    return y\n",
        "\n",
        "#CREATES A CELL AUTOMATON MODEL\n",
        "class CA(nn.Module):\n",
        "    def __init__(self, CHN):\n",
        "        super(CA, self).__init__()\n",
        "        self.w1 = nn.Conv2d(CHN, 128, 1)\n",
        "        self.w2 = nn.Conv2d(128, CHN, 1, bias=False)\n",
        "        nn.init.zeros_(self.w2.weight)  # Initialize weights to zero\n",
        "        repeat_values = torch.tensor([0.125, 0.25, 0.5, 1.0])\n",
        "        self.diff_coef = repeat_values.repeat_interleave(CHN // 4)\n",
        "\n",
        "    def get_diff_coef(self):\n",
        "        return self.diff_coef\n",
        "\n",
        "    def forward(self, x, r=1.0, d=1.0):\n",
        "        diff = laplacian(x) * self.get_diff_coef().view(1, -1, 1, 1)\n",
        "        y = F.sigmoid(self.w1(x) * 5.0)\n",
        "        react = self.w2(y)\n",
        "        x = x + diff * d + react * r\n",
        "        return x\n",
        "\n",
        "\n",
        "ca = CA(CHN=32)  # Make sure to pass the required arguments if there are any\n",
        "\n",
        "# Count parameters\n",
        "param_n = sum(p.numel() for p in ca.parameters() if p.requires_grad)\n",
        "print('Parameter count:', param_n)\n",
        "\n",
        "# Print the seed state examples\n",
        "print('Seed state examples:')\n",
        "\n",
        "# Generate seeds\n",
        "img = to_rgb(seed_f(4, 128))  # Generate and convert seeds to RGB\n",
        "\n",
        "# Pad and combine images for display\n",
        "img = np.pad(img, [(0, 0), (0, 0), (2, 2), (0, 0)], constant_values=1.0)  # Pad the images for visual separation\n",
        "combined_img = np.hstack(img.numpy())  # Combine images into a single array for display\n",
        "\n",
        "# Use imshow to display the images\n",
        "imshow(combined_img)\n"
      ],
      "metadata": {
        "id": "iSy9Ya9aV5SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image Dataset {vertical-output: false}\n",
        "\n",
        "# Load and prepare the target image\n",
        "def load_image(image_path, size=(96, 96)):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize(size)\n",
        "    tensor = ToTensor()(img)  # Converts to range [0, 1]\n",
        "    return tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "image_path = '/path/to/target/image.png'\n",
        "target = load_image(image_path)\n",
        "\n",
        "# Generate the initial seed state\n",
        "seed = seed_f(1, CHN=32)  # Adapt parameters as needed\n",
        "\n"
      ],
      "metadata": {
        "id": "UcZcKxt2q3M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and optimizer\n",
        "model = CA(CHN=32)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Prepare for loss tracking and video writing\n",
        "losses = []\n",
        "video_writer = VideoWriter(filename='training_process.mp4', fps=2)\n",
        "\n",
        "try:\n",
        "    epochs = 500\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(seed)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Record the loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Periodically update the loss plot and output image\n",
        "        if epoch % 10 == 0:\n",
        "            clear_output(wait=True)\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "            # Show the loss plot\n",
        "            ax1.set_title(\"Training Loss\")\n",
        "            ax1.plot(losses, label='Loss')\n",
        "            ax1.set_xlabel('Epochs')\n",
        "            ax1.set_ylabel('Loss')\n",
        "            ax1.legend()\n",
        "\n",
        "            # Convert tensor to image and show\n",
        "            output_image = output.detach().cpu().numpy()[0].transpose(1, 2, 0)\n",
        "            ax2.set_title(\"Current Output\")\n",
        "            ax2.imshow(np.clip(output_image, 0, 1))\n",
        "            ax2.axis('off')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "            # Add current frame to the video\n",
        "            video_writer.add(output_image)\n",
        "finally:\n",
        "    video_writer.close()  # Ensure the video is saved properly\n"
      ],
      "metadata": {
        "id": "ekj00ljZGi33"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}