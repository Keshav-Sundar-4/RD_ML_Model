{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMardOwB56c5hED1Yo4/vPC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keshav-Sundar-4/RD_ML_Model/blob/main/Oregonator_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#NEED TO DEFINE TARGET IMAGE, SEED STATE IS INITIAL PARAMS\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qWvYQvIMCX9z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get Images\n",
        "\n",
        "!wget https://raw.githubusercontent.com/Keshav-Sundar-4/RD_ML_Model/main/Spiral_RD.jpeg\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uOKTfg22sTpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6cZxrRf1CI1r"
      },
      "outputs": [],
      "source": [
        "#@title Image Manipulation\n",
        "\n",
        "# Load the target image\n",
        "def load_target_image(image_path, device, resize=None):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    if resize:\n",
        "        image = image.resize(resize, Image.LANCZOS)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    target_image = transform(image).unsqueeze(0).to(device)\n",
        "    return target_image\n",
        "\n",
        "#Convert to Grayscale for improved Pattern extraction\n",
        "def convert_to_grayscale(image):\n",
        "    gray_image = 0.2989 * image[:, 0:1, :, :] + \\\n",
        "                 0.5870 * image[:, 1:2, :, :] + \\\n",
        "                 0.1140 * image[:, 2:3, :, :]\n",
        "    return gray_image\n",
        "\n",
        "\n",
        "# Load and preprocess target image\n",
        "image_path = 'Spiral_RD.jpeg'\n",
        "height, width = 128, 128  # Adjust as needed\n",
        "target_image = load_target_image(image_path, device, resize=(width, height))\n",
        "target_image_gray = convert_to_grayscale(target_image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Oregonator Model Definition\n",
        "\n",
        "# Define the model\n",
        "class OregonatorModel(nn.Module):\n",
        "    def __init__(self, initial_params, device):\n",
        "        super(OregonatorModel, self).__init__()\n",
        "        # Trainable parameters derived from concentrations\n",
        "        self.epsilon = nn.Parameter(torch.tensor(initial_params['epsilon'], device=device))\n",
        "        self.f = nn.Parameter(torch.tensor(initial_params['f'], device=device))\n",
        "        self.q = nn.Parameter(torch.tensor(initial_params['q'], device=device))\n",
        "        self.D_x = nn.Parameter(torch.tensor(initial_params['D_x'], device=device))\n",
        "        self.D_z = nn.Parameter(torch.tensor(initial_params['D_z'], device=device))\n",
        "\n",
        "        self.device = device\n",
        "        self.laplacian_kernel = self.get_laplacian_kernel().to(device)\n",
        "\n",
        "    def get_laplacian_kernel(self):\n",
        "        kernel = torch.tensor([[0.05, 0.2, 0.05],\n",
        "                               [0.2, -1.0, 0.2],\n",
        "                               [0.05, 0.2, 0.05]], dtype=torch.float32)\n",
        "        kernel = kernel.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, 3, 3)\n",
        "        return kernel\n",
        "\n",
        "    def laplacian(self, u):\n",
        "        u_pad = F.pad(u, (1, 1, 1, 1), mode='circular')  # Wrap-around boundary conditions\n",
        "        delta_u = F.conv2d(u_pad, self.laplacian_kernel, padding=0)\n",
        "        return delta_u\n",
        "\n",
        "    def forward(self, x, z, dt):\n",
        "        delta_x = self.laplacian(x)\n",
        "        delta_z = self.laplacian(z)\n",
        "\n",
        "        # Reaction terms with trainable parameters\n",
        "        reaction_x = x * (1 - x) - self.f * ((x - self.q) / (x + self.q + 1e-8)) * z\n",
        "        reaction_z = x - z\n",
        "\n",
        "        # Update x and z with trainable epsilon and diffusion coefficients\n",
        "        x = x + dt * (reaction_x / self.epsilon + self.D_x * delta_x)\n",
        "        z = z + dt * (reaction_z + self.D_z * delta_z)\n",
        "\n",
        "        return x, z\n",
        "\n",
        "class InitialConcentrations(nn.Module):\n",
        "    def __init__(self, height, width, device):\n",
        "        super(InitialConcentrations, self).__init__()\n",
        "        # Initialize x and z with random values and make them trainable\n",
        "        self.x0 = nn.Parameter(torch.rand(1, 1, height, width, device=device))\n",
        "        self.z0 = nn.Parameter(torch.rand(1, 1, height, width, device=device))\n",
        "\n",
        "    def forward(self):\n",
        "        return self.x0, self.z0\n",
        "\n",
        "def variables_to_image(x, z):\n",
        "    # Normalize x and z to [0, 1]\n",
        "    x_norm = (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "    z_norm = (z - z.min()) / (z.max() - z.min() + 1e-8)\n",
        "\n",
        "    # Combine x and z to create a grayscale image\n",
        "    gray = 0.5 * (x_norm + z_norm)\n",
        "    return gray  # Shape: (batch_size, 1, height, width)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q5zxcgq5pCzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loss Functions\n",
        "\n",
        "class TextureLoss(nn.Module):\n",
        "    def __init__(self, target_image):\n",
        "        super(TextureLoss, self).__init__()\n",
        "        self.vgg = models.vgg19(pretrained=True).features[:21].eval()  # Up to 'conv4_1'\n",
        "        self.vgg.to(device)\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Preprocess target image\n",
        "        self.target_grams = self.get_grams(target_image)\n",
        "\n",
        "    def get_features(self, x):\n",
        "        x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        features = []\n",
        "        for layer in self.vgg:\n",
        "            x = layer(x)\n",
        "            if isinstance(layer, nn.ReLU):\n",
        "                features.append(x)\n",
        "        return features\n",
        "\n",
        "    def gram_matrix(self, feature):\n",
        "        (b, c, h, w) = feature.size()\n",
        "        features = feature.view(b, c, h * w)\n",
        "        G = torch.bmm(features, features.transpose(1, 2))\n",
        "        G = G / (c * h * w)\n",
        "        return G\n",
        "\n",
        "    def get_grams(self, x):\n",
        "        features = self.get_features(x)\n",
        "        grams = [self.gram_matrix(f) for f in features]\n",
        "        return grams\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_grams = self.get_grams(x)\n",
        "        loss = sum(F.mse_loss(g1, g2) for g1, g2 in zip(x_grams, self.target_grams))\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, target_image_gray):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.texture_loss = TextureLoss(target_image_gray)\n",
        "\n",
        "    def forward(self, output_image):\n",
        "        texture = self.texture_loss(output_image)\n",
        "        loss = texture\n",
        "        return loss"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AHQ5HrAup9KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parameter Intialization\n",
        "\n",
        "# Initialize model and optimizer\n",
        "initial_params = {\n",
        "    'epsilon': 0.02,\n",
        "    'f': 1.4,\n",
        "    'q': 0.002,\n",
        "    'D_x': 1.0,\n",
        "    'D_z': 0.5\n",
        "}\n",
        "model = OregonatorModel(initial_params, device).to(device)\n",
        "init_conc = InitialConcentrations(height, width, device).to(device)\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(init_conc.parameters()),\n",
        "    lr=1e-3\n",
        ")\n",
        "loss_fn = CombinedLoss(target_image_gray)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6geaEuMUqVS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Loop\n",
        "\n",
        "# Training parameters\n",
        "num_iterations = 1000\n",
        "time_steps = 100\n",
        "dt = 1.0\n",
        "loss_history = []\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    x_iter, z_iter = init_conc()\n",
        "    x_iter = torch.clamp(x_iter, 0.0, 1.0)\n",
        "    z_iter = torch.clamp(z_iter, 0.0, 1.0)\n",
        "\n",
        "    for _ in range(time_steps):\n",
        "        x_iter, z_iter = model(x_iter, z_iter, dt)\n",
        "        x_iter = torch.clamp(x_iter, 0.0, 1.0)\n",
        "        z_iter = torch.clamp(z_iter, 0.0, 1.0)\n",
        "\n",
        "    output_image = variables_to_image(x_iter, z_iter)\n",
        "    loss = loss_fn(output_image)\n",
        "    loss_value = loss.item()\n",
        "    loss_history.append(loss_value)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iteration % 50 == 0:\n",
        "        print(f\"Iteration {iteration}, Loss: {loss_value}\")\n",
        "        output_image_np = output_image.squeeze(0).detach().cpu().numpy()\n",
        "        plt.imsave(f'output_image_{iteration}.png', output_image_np[0], cmap='gray')\n",
        "\n",
        "# Plot loss curve\n",
        "plt.figure()\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.savefig('loss_curve.png')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4fMEy93erJ28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}